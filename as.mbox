From 1f1e366702f40551c0a0d2c4170614229ca8ce57 Mon Sep 17 00:00:00 2001
From: Yang Bo <bo@hyper.sh>
Date: Thu, 20 Sep 2018 15:46:46 +0800
Subject: [PATCH 1/6] Add framework to support address space operation

Signed-off-by: Yang Bo <bo@hyper.sh>
---
 arch/x86/entry/syscalls/syscall_64.tbl |  6 ++++
 mm/Makefile                            |  2 ++
 mm/as.c                                | 50 ++++++++++++++++++++++++++
 3 files changed, 58 insertions(+)
 create mode 100644 mm/as.c

diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 4dfe42666d0c..3ee0ebc2d6f1 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -341,6 +341,12 @@
 330	common	pkey_alloc		__x64_sys_pkey_alloc
 331	common	pkey_free		__x64_sys_pkey_free
 332	common	statx			__x64_sys_statx
+333 common  as_create       __x64_sys_as_create
+334 common  as_mmap         __x64_sys_as_mmap
+335 common  as_munmap       __x64_sys_as_munmap
+336 common  as_mprotect     __x64_sys_as_mprotect
+337 common  as_switch_mm    __x64_sys_as_switch_mm
+338 common  as_destroy      __x64_sys_as_destroy
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/mm/Makefile b/mm/Makefile
index b4e54a9ae9c5..86767011896e 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -49,6 +49,8 @@ else
 	obj-y		+= bootmem.o
 endif
 
+obj-y += as.o
+
 obj-$(CONFIG_ADVISE_SYSCALLS)	+= fadvise.o
 ifdef CONFIG_MMU
 	obj-$(CONFIG_ADVISE_SYSCALLS)	+= madvise.o
diff --git a/mm/as.c b/mm/as.c
new file mode 100644
index 000000000000..45002ecee82b
--- /dev/null
+++ b/mm/as.c
@@ -0,0 +1,50 @@
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/sched/mm.h>
+#include <linux/syscalls.h>
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/smp.h>
+#include <linux/sem.h>
+#include <linux/msg.h>
+#include <linux/mman.h>
+#include <linux/stat.h>
+#include <linux/file.h>
+#include <linux/personality.h>
+#include <linux/random.h>
+#include <linux/uaccess.h>
+#include <asm/syscalls.h>
+
+struct mmap_info;
+
+SYSCALL_DEFINE0(as_create)
+{
+    return -ENOSYS;
+}
+
+SYSCALL_DEFINE2(as_mmap, unsigned int, fd, struct mmap_info __user *, info)
+{
+    return -ENOSYS;
+}
+
+SYSCALL_DEFINE3(as_munmap, unsigned  int, fd, unsigned long, addr, 
+                unsigned long, len)
+{
+    return -ENOSYS;
+}
+
+SYSCALL_DEFINE4(as_mprotect, unsigned int, fd, unsigned long, addr,
+                unsigned long, len, unsigned int, prot)
+{
+    return -ENOSYS;
+}
+
+SYSCALL_DEFINE1(as_switch_mm, unsigned int, fd)
+{
+    return -ENOSYS;
+}
+
+SYSCALL_DEFINE1(as_destroy, unsigned int, fd)
+{
+    return -ENOSYS;
+}
-- 
2.19.0.rc1


From 32295c9dfd8e785e98b36294d3e8decbdec37287 Mon Sep 17 00:00:00 2001
From: Yang Bo <bo@hyper.sh>
Date: Thu, 20 Sep 2018 19:01:58 +0800
Subject: [PATCH 2/6] Prepare APIs for address space m{,un}map.

Signed-off-by: Yang Bo <bo@hyper.sh>
---
 include/linux/mm.h       | 18 +++++++++++++++++
 include/linux/syscalls.h |  4 ++++
 mm/internal.h            |  5 +++++
 mm/mmap.c                | 43 ++++++++++++++++++++++++++++++----------
 mm/mprotect.c            | 16 ++++++++++-----
 mm/util.c                | 14 +++++++++----
 6 files changed, 80 insertions(+), 20 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 28477ff9cf04..2186d9c71759 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2223,12 +2223,21 @@ extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned lo
 extern unsigned long mmap_region(struct file *file, unsigned long addr,
 	unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
 	struct list_head *uf);
+extern unsigned long mmap_region2(struct mm_struct *mm, struct file *file,
+    unsigned long addr, unsigned long len, vm_flags_t vm_flags,
+    unsigned long pgoff, struct list_head *uf);
 extern unsigned long do_mmap(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot, unsigned long flags,
 	vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,
 	struct list_head *uf);
+extern unsigned long do_mmap2(struct mm_struct *mm, struct file *file,
+    unsigned long addr, unsigned long len, unsigned long prot,
+    unsigned long flags, vm_flags_t vm_flags, unsigned long pgoff,
+    unsigned long *populate, struct list_head *uf);
 extern int do_munmap(struct mm_struct *, unsigned long, size_t,
 		     struct list_head *uf);
+extern int do_mprotect_pkey2(struct mm_struct *mm, unsigned long start, size_t len,
+		unsigned long prot, int pkey);
 
 static inline unsigned long
 do_mmap_pgoff(struct file *file, unsigned long addr,
@@ -2239,6 +2248,15 @@ do_mmap_pgoff(struct file *file, unsigned long addr,
 	return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate, uf);
 }
 
+static inline unsigned long
+do_mmap_pgoff2(struct mm_struct *mm, struct file *file, unsigned long addr,
+	unsigned long len, unsigned long prot, unsigned long flags,
+	unsigned long pgoff, unsigned long *populate,
+	struct list_head *uf)
+{
+	return do_mmap2(mm, file, addr, len, prot, flags, 0, pgoff, populate, uf);
+}
+
 #ifdef CONFIG_MMU
 extern int __mm_populate(unsigned long addr, unsigned long len,
 			 int ignore_errors);
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 70fcda1a9049..48f75294ce1f 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -67,6 +67,7 @@ struct perf_event_attr;
 struct file_handle;
 struct sigaltstack;
 union bpf_attr;
+struct mm_struct;
 
 #include <linux/types.h>
 #include <linux/aio_abi.h>
@@ -1147,6 +1148,9 @@ static inline int ksys_fadvise64_64(int fd, loff_t offset, loff_t len,
 unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
 			      unsigned long prot, unsigned long flags,
 			      unsigned long fd, unsigned long pgoff);
+unsigned long ksys_mmap_pgoff2(struct mm_struct *mm, unsigned long addr,
+                  unsigned long len, unsigned long prot, unsigned long flags,
+			      unsigned long fd, unsigned long pgoff);
 ssize_t ksys_readahead(int fd, loff_t offset, size_t count);
 
 /*
diff --git a/mm/internal.h b/mm/internal.h
index 502d14189794..2cd4a59067b6 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -469,6 +469,11 @@ extern unsigned long  __must_check vm_mmap_pgoff(struct file *, unsigned long,
         unsigned long, unsigned long,
         unsigned long, unsigned long);
 
+extern unsigned long  __must_check vm_mmap_pgoff2(struct mm_struct *mm,
+        struct file *, unsigned long,
+        unsigned long, unsigned long,
+        unsigned long, unsigned long);
+
 extern void set_pageblock_order(void);
 unsigned long reclaim_clean_pages_from_list(struct zone *zone,
 					    struct list_head *page_list);
diff --git a/mm/mmap.c b/mm/mmap.c
index 55d68c24e742..481c47b7a915 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1356,13 +1356,12 @@ static inline bool file_mmap_ok(struct file *file, struct inode *inode,
 /*
  * The caller must hold down_write(&current->mm->mmap_sem).
  */
-unsigned long do_mmap(struct file *file, unsigned long addr,
-			unsigned long len, unsigned long prot,
+unsigned long do_mmap2(struct mm_struct *mm, struct file *file,
+            unsigned long addr, unsigned long len, unsigned long prot,
 			unsigned long flags, vm_flags_t vm_flags,
 			unsigned long pgoff, unsigned long *populate,
 			struct list_head *uf)
 {
-	struct mm_struct *mm = current->mm;
 	int pkey = 0;
 
 	*populate = 0;
@@ -1532,7 +1531,7 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			vm_flags |= VM_NORESERVE;
 	}
 
-	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf);
+	addr = mmap_region2(mm, file, addr, len, vm_flags, pgoff, uf);
 	if (!IS_ERR_VALUE(addr) &&
 	    ((vm_flags & VM_LOCKED) ||
 	     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
@@ -1540,8 +1539,18 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	return addr;
 }
 
-unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
-			      unsigned long prot, unsigned long flags,
+unsigned long do_mmap(struct file *file, unsigned long addr,
+            unsigned long len, unsigned long prot,
+			unsigned long flags, vm_flags_t vm_flags,
+			unsigned long pgoff, unsigned long *populate,
+			struct list_head *uf)
+{
+    return do_mmap2(current->mm, file, addr, len, prot, flags, vm_flags,
+                    pgoff, populate, uf);
+}
+
+unsigned long ksys_mmap_pgoff2(struct mm_struct *mm, unsigned long addr,
+                  unsigned long len, unsigned long prot, unsigned long flags,
 			      unsigned long fd, unsigned long pgoff)
 {
 	struct file *file = NULL;
@@ -1582,13 +1591,20 @@ unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
 
 	flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);
 
-	retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);
+	retval = vm_mmap_pgoff2(mm, file, addr, len, prot, flags, pgoff);
 out_fput:
 	if (file)
 		fput(file);
 	return retval;
 }
 
+unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
+			      unsigned long prot, unsigned long flags,
+			      unsigned long fd, unsigned long pgoff)
+{
+    return ksys_mmap_pgoff2(current->mm, addr, len, prot, flags, fd, pgoff);
+}
+
 SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
 		unsigned long, prot, unsigned long, flags,
 		unsigned long, fd, unsigned long, pgoff)
@@ -1674,11 +1690,10 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)
 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
 }
 
-unsigned long mmap_region(struct file *file, unsigned long addr,
-		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
-		struct list_head *uf)
+unsigned long mmap_region2(struct mm_struct *mm, struct file *file,
+        unsigned long addr, unsigned long len, vm_flags_t vm_flags,
+        unsigned long pgoff, struct list_head *uf)
 {
-	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma, *prev;
 	int error;
 	struct rb_node **rb_link, *rb_parent;
@@ -1837,6 +1852,12 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	return error;
 }
 
+unsigned long mmap_region(struct file *file, unsigned long addr,
+		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
+		struct list_head *uf)
+{
+    return mmap_region2(current->mm, file, addr, len, vm_flags, pgoff, uf);
+}
 unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 {
 	/*
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 6d331620b9e5..cbe12bd05287 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -453,7 +453,7 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 /*
  * pkey==-1 when doing a legacy mprotect()
  */
-static int do_mprotect_pkey(unsigned long start, size_t len,
+int do_mprotect_pkey2(struct mm_struct *mm, unsigned long start, size_t len,
 		unsigned long prot, int pkey)
 {
 	unsigned long nstart, end, tmp, reqprot;
@@ -480,7 +480,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 
 	reqprot = prot;
 
-	if (down_write_killable(&current->mm->mmap_sem))
+	if (down_write_killable(&mm->mmap_sem))
 		return -EINTR;
 
 	/*
@@ -488,10 +488,10 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 	 * them use it here.
 	 */
 	error = -EINVAL;
-	if ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))
+	if ((pkey != -1) && !mm_pkey_is_allocated(mm, pkey))
 		goto out;
 
-	vma = find_vma(current->mm, start);
+	vma = find_vma(mm, start);
 	error = -ENOMEM;
 	if (!vma)
 		goto out;
@@ -570,10 +570,16 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		prot = reqprot;
 	}
 out:
-	up_write(&current->mm->mmap_sem);
+	up_write(&mm->mmap_sem);
 	return error;
 }
 
+static int do_mprotect_pkey(unsigned long start, size_t len,
+		unsigned long prot, int pkey)
+{
+    return do_mprotect_pkey2(current->mm, start, len, prot, pkey);
+}
+
 SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 		unsigned long, prot)
 {
diff --git a/mm/util.c b/mm/util.c
index 45fc3169e7b0..66afd4f9f7f2 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -341,12 +341,11 @@ int __weak get_user_pages_fast(unsigned long start,
 }
 EXPORT_SYMBOL_GPL(get_user_pages_fast);
 
-unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
-	unsigned long len, unsigned long prot,
+unsigned long vm_mmap_pgoff2(struct mm_struct *mm, struct file *file,
+    unsigned long addr, unsigned long len, unsigned long prot,
 	unsigned long flag, unsigned long pgoff)
 {
 	unsigned long ret;
-	struct mm_struct *mm = current->mm;
 	unsigned long populate;
 	LIST_HEAD(uf);
 
@@ -354,7 +353,7 @@ unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
 	if (!ret) {
 		if (down_write_killable(&mm->mmap_sem))
 			return -EINTR;
-		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,
+		ret = do_mmap_pgoff2(mm, file, addr, len, prot, flag, pgoff,
 				    &populate, &uf);
 		up_write(&mm->mmap_sem);
 		userfaultfd_unmap_complete(mm, &uf);
@@ -364,6 +363,13 @@ unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
 	return ret;
 }
 
+unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
+	unsigned long len, unsigned long prot,
+	unsigned long flag, unsigned long pgoff)
+{
+    return vm_mmap_pgoff2(current->mm, file, addr, len, prot, flag, pgoff);
+}
+
 unsigned long vm_mmap(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot,
 	unsigned long flag, unsigned long offset)
-- 
2.19.0.rc1


From 2864a20672cc3988298816175bb857223c7d271e Mon Sep 17 00:00:00 2001
From: Yang Bo <bo@hyper.sh>
Date: Fri, 21 Sep 2018 14:50:42 +0800
Subject: [PATCH 3/6] Implement address space syscalls.

Signed-off-by: Yang Bo <bo@hyper.sh>
---
 mm/as.c | 183 +++++++++++++++++++++++++++++++++++++++++++++++++++++---
 mm/as.h |  18 ++++++
 2 files changed, 193 insertions(+), 8 deletions(-)
 create mode 100644 mm/as.h

diff --git a/mm/as.c b/mm/as.c
index 45002ecee82b..016fc7eb5547 100644
--- a/mm/as.c
+++ b/mm/as.c
@@ -13,38 +13,205 @@
 #include <linux/personality.h>
 #include <linux/random.h>
 #include <linux/uaccess.h>
+#include <linux/mmu_context.h>
+#include <linux/vmacache.h>
+#include <linux/anon_inodes.h>
 #include <asm/syscalls.h>
+#include <asm/mmu_context.h>
+#include "as.h"
 
-struct mmap_info;
+// static long mm_nr = 0;
+
+static int as_release(struct inode *inode, struct file *file);
+struct file_operations as_fops = {
+    .release = as_release,
+    };
+
+
+static struct mm_struct *get_mm_from_fd(int fd)
+{
+    struct file *f;
+    struct mm_struct *mm = ERR_PTR(-EBADF);
+
+    f = fget(fd);
+    if (!f) {
+        return mm;
+    }
+
+    if (f->f_op != &as_fops) {
+        mm = ERR_PTR(-EINVAL);
+        goto out;
+    }
+
+    mm = f->private_data;
+
+out:
+    fput(f);
+    return mm;
+}
 
 SYSCALL_DEFINE0(as_create)
 {
-    return -ENOSYS;
+    int fd;
+    struct mm_struct *mm;
+
+    mm = mm_alloc();
+    if (!mm) {
+        return -ENOMEM;
+    }
+
+    fd = anon_inode_getfd("[adress-space]", &as_fops, mm, 0);
+    if (fd < 0) {
+        mmput(mm);
+        return fd;
+    }
+
+
+    /* add to mmlist? */
+    spin_lock(&mmlist_lock);
+    list_add(&mm->mmlist, &current->mm->mmlist);
+    /* looks like we will never go back to the initial mm..
+     * Therefore, let it vanish
+     * */
+#if 0
+    if (!mm_nr) {
+        mmget(current->mm);
+        mm_nr++;
+    }
+#endif
+    spin_unlock(&mmlist_lock);
+
+    return fd;
 }
 
 SYSCALL_DEFINE2(as_mmap, unsigned int, fd, struct mmap_info __user *, info)
 {
-    return -ENOSYS;
+    struct mmap_info kinfo;
+    struct mm_struct *mm;
+
+    if (copy_from_user(&kinfo, info, sizeof(struct mmap_info))) {
+        return -EFAULT;
+    }
+
+    mm = get_mm_from_fd(fd);
+    if (IS_ERR(mm)) {
+        return PTR_ERR(mm);
+    }
+
+    return ksys_mmap_pgoff2(mm, kinfo.addr, kinfo.len, kinfo.prot, kinfo.flags,
+                                kinfo.fd, kinfo.offset >> PAGE_SHIFT);
 }
 
-SYSCALL_DEFINE3(as_munmap, unsigned  int, fd, unsigned long, addr, 
+SYSCALL_DEFINE3(as_munmap, unsigned  int, fd, unsigned long, addr,
                 unsigned long, len)
 {
-    return -ENOSYS;
+    struct mm_struct *mm;
+    int ret;
+
+    mm = get_mm_from_fd(fd);
+    if (IS_ERR(mm)) {
+        return PTR_ERR(mm);
+    }
+
+    if (down_write_killable(&mm->mmap_sem)) {
+        return -EINTR;
+    }
+
+    ret = do_munmap(mm, addr, len, NULL);
+
+    up_write(&mm->mmap_sem);
+
+    return ret;
 }
 
 SYSCALL_DEFINE4(as_mprotect, unsigned int, fd, unsigned long, addr,
                 unsigned long, len, unsigned int, prot)
 {
-    return -ENOSYS;
+    struct mm_struct *mm;
+
+    mm = get_mm_from_fd(fd);
+    if (IS_ERR(mm)) {
+        return PTR_ERR(mm);
+    }
+
+    return do_mprotect_pkey2(mm, addr, len, prot, -1);;
 }
 
 SYSCALL_DEFINE1(as_switch_mm, unsigned int, fd)
 {
-    return -ENOSYS;
+    struct mm_struct *mm, *old_mm, *active_mm;
+    struct task_struct *tsk = current;
+
+    mm = get_mm_from_fd(fd);
+    if (IS_ERR(mm)) {
+        return PTR_ERR(mm);
+    }
+
+    old_mm = tsk->mm;
+
+    if (mm == old_mm || mm == tsk->active_mm) {
+        return 0;
+    }
+
+    mm_release(tsk, old_mm);
+
+    /* we are not kernel thread.
+     * we definitely have task->mm and task->active_mm..
+     * and mm == active_mm.
+     * */
+    if (old_mm) {
+        sync_mm_rss(old_mm);
+        down_read(&old_mm->mmap_sem);
+        mmget(mm);
+    } else {
+        mmgrab(mm);
+    }
+
+    task_lock(tsk);
+    active_mm = tsk->active_mm;
+    tsk->mm = mm;
+    tsk->active_mm = mm;
+    activate_mm(active_mm, mm);
+    tsk->mm->vmacache_seqnum = 0;
+    vmacache_flush(tsk);
+    task_unlock(tsk);
+
+    /* simply mmput/mmdrop will cause the initial mm of this task
+     * vanish.. hmmm.. Not sure if need to keep it.
+     * */
+    if (old_mm) {
+        up_read(&old_mm->mmap_sem);
+        BUG_ON(active_mm != old_mm);
+        setmax_mm_hiwater_rss(&tsk->signal->maxrss, old_mm);
+        mm_update_next_owner(old_mm);
+        mmput(old_mm);
+        return 0;
+    }
+    
+    mmdrop(active_mm);
+    return 0;
 }
 
 SYSCALL_DEFINE1(as_destroy, unsigned int, fd)
 {
-    return -ENOSYS;
+    struct mm_struct *mm;
+    struct task_struct *task;
+
+    task = current;
+    mm = get_mm_from_fd(fd);
+    if (IS_ERR(mm)) {
+        return PTR_ERR(mm);
+    }
+
+    __close_fd(current->files, fd);
+
+    return 0;
+}
+
+static int as_release(struct inode *inode, struct file *file)
+{
+    struct mm_struct *mm = file->private_data;
+
+    mmput(mm);
+    return 0;
 }
diff --git a/mm/as.h b/mm/as.h
new file mode 100644
index 000000000000..4542c9b28ec5
--- /dev/null
+++ b/mm/as.h
@@ -0,0 +1,18 @@
+#ifndef ___AS__H
+#define ___AS__H
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+
+extern struct file *get_empty_filp(void);
+
+struct mmap_info {
+    unsigned long addr;
+    unsigned long len;
+    unsigned long prot;
+    unsigned long flags;
+    unsigned long fd;
+    unsigned long offset;
+};
+
+#endif
-- 
2.19.0.rc1


From 0eac0deec9a963d2cde03546f200ec03ed630d4d Mon Sep 17 00:00:00 2001
From: Yang Bo <bo@hyper.sh>
Date: Tue, 25 Sep 2018 13:47:05 +0800
Subject: [PATCH 4/6] Add as_copy for test purpose.

Signed-off-by: Yang Bo <bo@hyper.sh>
---
 arch/x86/entry/syscalls/syscall_64.tbl |   1 +
 kernel/fork.c                          |   2 +-
 mm/as.c                                | 135 ++++++++++++++++++++++++-
 mm/as.h                                |   1 +
 4 files changed, 134 insertions(+), 5 deletions(-)

diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 3ee0ebc2d6f1..3e9a24fae208 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -347,6 +347,7 @@
 336 common  as_mprotect     __x64_sys_as_mprotect
 337 common  as_switch_mm    __x64_sys_as_switch_mm
 338 common  as_destroy      __x64_sys_as_destroy
+339 common  as_copy         __x64_sys_as_copy
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/kernel/fork.c b/kernel/fork.c
index b9d9b39d4afc..e3bb7a62470e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1250,7 +1250,7 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
  * Allocate a new mm structure and copy contents from the
  * mm structure of the passed in task structure.
  */
-static struct mm_struct *dup_mm(struct task_struct *tsk)
+struct mm_struct *dup_mm(struct task_struct *tsk)
 {
 	struct mm_struct *mm, *oldmm = current->mm;
 	int err;
diff --git a/mm/as.c b/mm/as.c
index 016fc7eb5547..fae2b7da58b4 100644
--- a/mm/as.c
+++ b/mm/as.c
@@ -20,12 +20,18 @@
 #include <asm/mmu_context.h>
 #include "as.h"
 
+
+#define DEBUG
 // static long mm_nr = 0;
 
 static int as_release(struct inode *inode, struct file *file);
+#ifdef DEBUG
+static void dump_pt_regs(struct pt_regs *reg);
+#endif
+static long do_as_switch_mm(unsigned int fd);
 struct file_operations as_fops = {
     .release = as_release,
-    };
+};
 
 
 static struct mm_struct *get_mm_from_fd(int fd)
@@ -66,6 +72,46 @@ SYSCALL_DEFINE0(as_create)
         return fd;
     }
 
+    /* add to mmlist? */
+    spin_lock(&mmlist_lock);
+    list_add(&mm->mmlist, &current->mm->mmlist);
+    /* looks like we will never go back to the initial mm..
+     * Therefore, let it vanish
+     * */
+#if 0
+    if (!mm_nr) {
+        mmget(current->mm);
+        mm_nr++;
+    }
+#endif
+    spin_unlock(&mmlist_lock);
+
+    return fd;
+}
+
+/* copy mm struct from current mm struct.
+ * used for test code. Or might be usefull..
+ * However, this is COW. Therfore, If we switch to
+ * this mm later, it results in weird execution path
+ * and segv..
+ * */
+SYSCALL_DEFINE0(as_copy)
+{
+    int fd, ret;
+    struct mm_struct *mm;
+    struct task_struct *task;
+
+    task = current;
+    mm = dup_mm(task);
+    if (!mm) {
+        return -ENOMEM;
+    }
+
+    fd = anon_inode_getfd("[adress-space]", &as_fops, mm, 0);
+    if (fd < 0) {
+        mmput(mm);
+        return fd;
+    }
 
     /* add to mmlist? */
     spin_lock(&mmlist_lock);
@@ -81,6 +127,14 @@ SYSCALL_DEFINE0(as_create)
 #endif
     spin_unlock(&mmlist_lock);
 
+#ifdef DEBUG
+    ret = do_as_switch_mm(fd);
+    if (ret) {
+        __close_fd(current->files, fd);
+        return ret;
+    }
+#endif
+
     return fd;
 }
 
@@ -137,11 +191,25 @@ SYSCALL_DEFINE4(as_mprotect, unsigned int, fd, unsigned long, addr,
     return do_mprotect_pkey2(mm, addr, len, prot, -1);;
 }
 
-SYSCALL_DEFINE1(as_switch_mm, unsigned int, fd)
+static long do_as_switch_mm(unsigned int fd)
 {
     struct mm_struct *mm, *old_mm, *active_mm;
     struct task_struct *tsk = current;
-
+    struct pt_regs *old, *new;
+    mm_segment_t oldfs;
+    unsigned long oldfs2, oldgs;
+    unsigned int fsindex, gsindex;
+
+    oldfs = get_fs();
+    rdmsrl(MSR_FS_BASE, oldfs2);
+    rdmsrl(MSR_KERNEL_GS_BASE, oldgs);
+    savesegment(fs, fsindex);
+    savesegment(gs, gsindex);
+#ifdef DEBUG
+    printk(KERN_INFO "oldfs[%lx], fsbase[%lx], gsbase[%lx]\n", oldfs.seg,
+            oldfs2, oldgs);
+    printk(KERN_INFO "fsindex[%x], gsindex[%x]\n", fsindex, gsindex);
+#endif
     mm = get_mm_from_fd(fd);
     if (IS_ERR(mm)) {
         return PTR_ERR(mm);
@@ -153,6 +221,14 @@ SYSCALL_DEFINE1(as_switch_mm, unsigned int, fd)
         return 0;
     }
 
+#ifdef DEBUG
+    /* check pt_regs before and after switch mm.
+     * looks like the return address is messed up.. 
+     * */
+    old = task_pt_regs(current);
+    dump_pt_regs(old);
+#endif
+
     mm_release(tsk, old_mm);
 
     /* we are not kernel thread.
@@ -179,19 +255,36 @@ SYSCALL_DEFINE1(as_switch_mm, unsigned int, fd)
     /* simply mmput/mmdrop will cause the initial mm of this task
      * vanish.. hmmm.. Not sure if need to keep it.
      * */
+#ifdef DEBUG
+    new = task_pt_regs(current);
+    dump_pt_regs(new);
+#endif
     if (old_mm) {
         up_read(&old_mm->mmap_sem);
         BUG_ON(active_mm != old_mm);
         setmax_mm_hiwater_rss(&tsk->signal->maxrss, old_mm);
         mm_update_next_owner(old_mm);
         mmput(old_mm);
-        return 0;
+        goto out;
     }
     
     mmdrop(active_mm);
+
+out:
+//    set_fs(oldfs);
+    set_fs(USER_DS);
+    wrmsrl(MSR_FS_BASE, oldfs2);
+    wrmsrl(MSR_KERNEL_GS_BASE, oldgs);
+    loadsegment(fs, fsindex);
+    load_gs_index(gsindex);
     return 0;
 }
 
+SYSCALL_DEFINE1(as_switch_mm, unsigned int, fd)
+{
+    return do_as_switch_mm(fd);
+}
+
 SYSCALL_DEFINE1(as_destroy, unsigned int, fd)
 {
     struct mm_struct *mm;
@@ -215,3 +308,37 @@ static int as_release(struct inode *inode, struct file *file)
     mmput(mm);
     return 0;
 }
+
+#ifdef DEBUG
+static void dump_pt_regs(struct pt_regs *reg)
+{
+    int i = 0;
+    char *c, ch;
+    printk(KERN_INFO "r15[%lx], r14[%lx], r13[%lx], r12[%lx]\n",
+            reg->r15, reg->r14, reg->r13, reg->r12);
+    printk(KERN_INFO "bp[%lx], bx[%lx], r11[%lx], r10[%lx]\n",
+            reg->bp, reg->bx, reg->r11, reg->r10);
+    printk(KERN_INFO "r9[%lx], r8[%lx], ax[%lx], cx[%lx]\n",
+            reg->r9, reg->r8, reg->ax, reg->cx);
+    printk(KERN_INFO "dx[%lx], si[%lx], di[%lx], orig_ax[%lx]\n",
+            reg->dx, reg->si, reg->di, reg->orig_ax);
+    printk(KERN_INFO "ip[%lx], cs[%lx], flags[%lx], sp[%lx]\n",
+            reg->ip, reg->cs, reg->flags, reg->sp);
+    printk(KERN_INFO "ss[%lx]\n", reg->ss);
+#if 0
+    printk(KERN_INFO "dump instructions:\n");
+    c = (char *)reg->ip;
+    for (i = 0; i < 50; i++, c++) {
+        if (get_user(ch, c)) {
+            printk(KERN_INFO "bug ");
+            continue;
+        }
+        printk(KERN_INFO "%02x ", ch);
+        if (i % 10 == 9) {
+            printk(KERN_INFO "\n");
+        }
+    }
+#endif
+    return;
+}
+#endif
diff --git a/mm/as.h b/mm/as.h
index 4542c9b28ec5..dd4646bcdcf7 100644
--- a/mm/as.h
+++ b/mm/as.h
@@ -5,6 +5,7 @@
 #include <linux/kernel.h>
 
 extern struct file *get_empty_filp(void);
+extern struct mm_struct *dup_mm(struct task_struct *task);
 
 struct mmap_info {
     unsigned long addr;
-- 
2.19.0.rc1


From d84aafed199e8f0a5e0a62a8da368cbf40f54d0d Mon Sep 17 00:00:00 2001
From: Yang Bo <bo@hyper.sh>
Date: Wed, 26 Sep 2018 16:09:55 +0800
Subject: [PATCH 5/6] Add copy_page_rang_nocow.

The not-copy-on-write is exactly what we need here.

Signed-off-by: Yang Bo <bo@hyper.sh>
---
 include/linux/huge_mm.h |   6 +
 include/linux/hugetlb.h |   1 +
 include/linux/mm.h      |   2 +
 kernel/fork.c           | 188 ++++++++++++++++++++++
 mm/as.h                 |   2 +-
 mm/huge_memory.c        | 124 +++++++++++++++
 mm/hugetlb.c            |  89 +++++++++++
 mm/memory.c             | 337 ++++++++++++++++++++++++++++++++++++++++
 8 files changed, 748 insertions(+), 1 deletion(-)

diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index a8a126259bc4..ad041b304899 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -10,10 +10,16 @@ extern int do_huge_pmd_anonymous_page(struct vm_fault *vmf);
 extern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			 pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
 			 struct vm_area_struct *vma);
+extern int copy_huge_pmd_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+			 pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
+			 struct vm_area_struct *vma);
 extern void huge_pmd_set_accessed(struct vm_fault *vmf, pmd_t orig_pmd);
 extern int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			 pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
 			 struct vm_area_struct *vma);
+extern int copy_huge_pud_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+			 pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
+			 struct vm_area_struct *vma);
 
 #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
 extern void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud);
diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 36fa6a2a82e3..1a8e0f28cd33 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -88,6 +88,7 @@ int hugetlb_mempolicy_sysctl_handler(struct ctl_table *, int,
 #endif
 
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
+int copy_hugetlb_page_range_nocow(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
 long follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *,
 			 struct page **, struct vm_area_struct **,
 			 unsigned long *, unsigned long *, long, unsigned int,
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 2186d9c71759..5ac825eeda5d 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1327,6 +1327,8 @@ void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 		unsigned long end, unsigned long floor, unsigned long ceiling);
 int copy_page_range(struct mm_struct *dst, struct mm_struct *src,
 			struct vm_area_struct *vma);
+int copy_page_range_nocow(struct mm_struct *dst, struct mm_struct *src,
+			struct vm_area_struct *vma);
 int follow_pte_pmd(struct mm_struct *mm, unsigned long address,
 			     unsigned long *start, unsigned long *end,
 			     pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp);
diff --git a/kernel/fork.c b/kernel/fork.c
index e3bb7a62470e..20dd0125622c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -571,6 +571,158 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	goto out;
 }
 
+static __latent_entropy int dup_mmap_nocow(struct mm_struct *mm,
+					struct mm_struct *oldmm)
+{
+	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
+	struct rb_node **rb_link, *rb_parent;
+	int retval;
+	unsigned long charge;
+	LIST_HEAD(uf);
+
+	uprobe_start_dup_mmap();
+	if (down_write_killable(&oldmm->mmap_sem)) {
+		retval = -EINTR;
+		goto fail_uprobe_end;
+	}
+	flush_cache_dup_mm(oldmm);
+	uprobe_dup_mmap(oldmm, mm);
+	/*
+	 * Not linked in yet - no deadlock potential:
+	 */
+	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
+
+	/* No ordering required: file already has been exposed. */
+	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
+
+	mm->total_vm = oldmm->total_vm;
+	mm->data_vm = oldmm->data_vm;
+	mm->exec_vm = oldmm->exec_vm;
+	mm->stack_vm = oldmm->stack_vm;
+
+	rb_link = &mm->mm_rb.rb_node;
+	rb_parent = NULL;
+	pprev = &mm->mmap;
+	retval = ksm_fork(mm, oldmm);
+	if (retval)
+		goto out;
+	retval = khugepaged_fork(mm, oldmm);
+	if (retval)
+		goto out;
+
+	prev = NULL;
+	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
+		struct file *file;
+
+		if (mpnt->vm_flags & VM_DONTCOPY) {
+			vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
+			continue;
+		}
+		charge = 0;
+		/*
+		 * Don't duplicate many vmas if we've been oom-killed (for
+		 * example)
+		 */
+		if (fatal_signal_pending(current)) {
+			retval = -EINTR;
+			goto out;
+		}
+		if (mpnt->vm_flags & VM_ACCOUNT) {
+			unsigned long len = vma_pages(mpnt);
+
+			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
+				goto fail_nomem;
+			charge = len;
+		}
+		tmp = vm_area_dup(mpnt);
+		if (!tmp)
+			goto fail_nomem;
+		retval = vma_dup_policy(mpnt, tmp);
+		if (retval)
+			goto fail_nomem_policy;
+		tmp->vm_mm = mm;
+		retval = dup_userfaultfd(tmp, &uf);
+		if (retval)
+			goto fail_nomem_anon_vma_fork;
+		if (tmp->vm_flags & VM_WIPEONFORK) {
+			/* VM_WIPEONFORK gets a clean slate in the child. */
+			tmp->anon_vma = NULL;
+			if (anon_vma_prepare(tmp))
+				goto fail_nomem_anon_vma_fork;
+		} else if (anon_vma_fork(tmp, mpnt))
+			goto fail_nomem_anon_vma_fork;
+		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);
+		tmp->vm_next = tmp->vm_prev = NULL;
+		file = tmp->vm_file;
+		if (file) {
+			struct inode *inode = file_inode(file);
+			struct address_space *mapping = file->f_mapping;
+
+			get_file(file);
+			if (tmp->vm_flags & VM_DENYWRITE)
+				atomic_dec(&inode->i_writecount);
+			i_mmap_lock_write(mapping);
+			if (tmp->vm_flags & VM_SHARED)
+				atomic_inc(&mapping->i_mmap_writable);
+			flush_dcache_mmap_lock(mapping);
+			/* insert tmp into the share list, just after mpnt */
+			vma_interval_tree_insert_after(tmp, mpnt,
+					&mapping->i_mmap);
+			flush_dcache_mmap_unlock(mapping);
+			i_mmap_unlock_write(mapping);
+		}
+
+		/*
+		 * Clear hugetlb-related page reserves for children. This only
+		 * affects MAP_PRIVATE mappings. Faults generated by the child
+		 * are not guaranteed to succeed, even if read-only
+		 */
+		if (is_vm_hugetlb_page(tmp))
+			reset_vma_resv_huge_pages(tmp);
+
+		/*
+		 * Link in the new vma and copy the page table entries.
+		 */
+		*pprev = tmp;
+		pprev = &tmp->vm_next;
+		tmp->vm_prev = prev;
+		prev = tmp;
+
+		__vma_link_rb(mm, tmp, rb_link, rb_parent);
+		rb_link = &tmp->vm_rb.rb_right;
+		rb_parent = &tmp->vm_rb;
+
+		mm->map_count++;
+		if (!(tmp->vm_flags & VM_WIPEONFORK))
+			retval = copy_page_range_nocow(mm, oldmm, mpnt);
+
+		if (tmp->vm_ops && tmp->vm_ops->open)
+			tmp->vm_ops->open(tmp);
+
+		if (retval)
+			goto out;
+	}
+	/* a new mm has just been created */
+	arch_dup_mmap(oldmm, mm);
+	retval = 0;
+out:
+	up_write(&mm->mmap_sem);
+	flush_tlb_mm(oldmm);
+	up_write(&oldmm->mmap_sem);
+	dup_userfaultfd_complete(&uf);
+fail_uprobe_end:
+	uprobe_end_dup_mmap();
+	return retval;
+fail_nomem_anon_vma_fork:
+	mpol_put(vma_policy(tmp));
+fail_nomem_policy:
+	vm_area_free(tmp);
+fail_nomem:
+	retval = -ENOMEM;
+	vm_unacct_memory(charge);
+	goto out;
+}
+
 static inline int mm_alloc_pgd(struct mm_struct *mm)
 {
 	mm->pgd = pgd_alloc(mm);
@@ -1285,6 +1437,42 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	return NULL;
 }
 
+/* dup mm without COW. */
+struct mm_struct *dup_mm_nocow(struct task_struct *tsk)
+{
+	struct mm_struct *mm, *oldmm = current->mm;
+	int err;
+
+	mm = allocate_mm();
+	if (!mm)
+		goto fail_nomem;
+
+	memcpy(mm, oldmm, sizeof(*mm));
+
+	if (!mm_init(mm, tsk, mm->user_ns))
+		goto fail_nomem;
+
+	err = dup_mmap_nocow(mm, oldmm);
+	if (err)
+		goto free_pt;
+
+	mm->hiwater_rss = get_mm_rss(mm);
+	mm->hiwater_vm = mm->total_vm;
+
+	if (mm->binfmt && !try_module_get(mm->binfmt->module))
+		goto free_pt;
+
+	return mm;
+
+free_pt:
+	/* don't put binfmt in mmput, we haven't got module yet */
+	mm->binfmt = NULL;
+	mmput(mm);
+
+fail_nomem:
+	return NULL;
+}
+
 static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct mm_struct *mm, *oldmm;
diff --git a/mm/as.h b/mm/as.h
index dd4646bcdcf7..165a6bb78ea6 100644
--- a/mm/as.h
+++ b/mm/as.h
@@ -5,7 +5,7 @@
 #include <linux/kernel.h>
 
 extern struct file *get_empty_filp(void);
-extern struct mm_struct *dup_mm(struct task_struct *task);
+extern struct mm_struct *dup_mm_nocow(struct task_struct *task);
 
 struct mmap_info {
     unsigned long addr;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 327e12679dd5..a1795f16616b 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -988,6 +988,95 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	return ret;
 }
 
+int copy_huge_pmd_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		  pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
+		  struct vm_area_struct *vma)
+{
+	spinlock_t *dst_ptl, *src_ptl;
+	struct page *src_page;
+	pmd_t pmd;
+	pgtable_t pgtable = NULL;
+	int ret = -ENOMEM;
+
+	/* Skip if can be re-fill on fault */
+	if (!vma_is_anonymous(vma))
+		return 0;
+
+	pgtable = pte_alloc_one(dst_mm, addr);
+	if (unlikely(!pgtable))
+		goto out;
+
+	dst_ptl = pmd_lock(dst_mm, dst_pmd);
+	src_ptl = pmd_lockptr(src_mm, src_pmd);
+	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
+
+	ret = -EAGAIN;
+	pmd = *src_pmd;
+
+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION
+	if (unlikely(is_swap_pmd(pmd))) {
+		swp_entry_t entry = pmd_to_swp_entry(pmd);
+
+		VM_BUG_ON(!is_pmd_migration_entry(pmd));
+#if 0
+		if (is_write_migration_entry(entry)) {
+			make_migration_entry_read(&entry);
+			pmd = swp_entry_to_pmd(entry);
+			if (pmd_swp_soft_dirty(*src_pmd))
+				pmd = pmd_swp_mksoft_dirty(pmd);
+			set_pmd_at(src_mm, addr, src_pmd, pmd);
+		}
+#endif
+		add_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);
+		mm_inc_nr_ptes(dst_mm);
+		pgtable_trans_huge_deposit(dst_mm, dst_pmd, pgtable);
+		set_pmd_at(dst_mm, addr, dst_pmd, pmd);
+		ret = 0;
+		goto out_unlock;
+	}
+#endif
+
+	if (unlikely(!pmd_trans_huge(pmd))) {
+		pte_free(dst_mm, pgtable);
+		goto out_unlock;
+	}
+	/*
+	 * When page table lock is held, the huge zero pmd should not be
+	 * under splitting since we don't split the page itself, only pmd to
+	 * a page table.
+	 */
+	if (is_huge_zero_pmd(pmd)) {
+		struct page *zero_page;
+		/*
+		 * get_huge_zero_page() will never allocate a new page here,
+		 * since we already have a zero page to copy. It just takes a
+		 * reference.
+		 */
+		zero_page = mm_get_huge_zero_page(dst_mm);
+		set_huge_zero_page(pgtable, dst_mm, vma, addr, dst_pmd,
+				zero_page);
+		ret = 0;
+		goto out_unlock;
+	}
+
+	src_page = pmd_page(pmd);
+	VM_BUG_ON_PAGE(!PageHead(src_page), src_page);
+	get_page(src_page);
+	page_dup_rmap(src_page, true);
+	add_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);
+	mm_inc_nr_ptes(dst_mm);
+	pgtable_trans_huge_deposit(dst_mm, dst_pmd, pgtable);
+
+	set_pmd_at(dst_mm, addr, dst_pmd, pmd);
+
+	ret = 0;
+out_unlock:
+	spin_unlock(src_ptl);
+	spin_unlock(dst_ptl);
+out:
+	return ret;
+}
+
 #ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD
 static void touch_pud(struct vm_area_struct *vma, unsigned long addr,
 		pud_t *pud, int flags)
@@ -1078,6 +1167,41 @@ int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	return ret;
 }
 
+int copy_huge_pud_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		  pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
+		  struct vm_area_struct *vma)
+{
+	spinlock_t *dst_ptl, *src_ptl;
+	pud_t pud;
+	int ret;
+
+	dst_ptl = pud_lock(dst_mm, dst_pud);
+	src_ptl = pud_lockptr(src_mm, src_pud);
+	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
+
+	ret = -EAGAIN;
+	pud = *src_pud;
+	if (unlikely(!pud_trans_huge(pud) && !pud_devmap(pud)))
+		goto out_unlock;
+
+	/*
+	 * When page table lock is held, the huge zero pud should not be
+	 * under splitting since we don't split the page itself, only pud to
+	 * a page table.
+	 */
+	if (is_huge_zero_pud(pud)) {
+		/* No huge zero pud yet */
+	}
+
+	set_pud_at(dst_mm, addr, dst_pud, pud);
+
+	ret = 0;
+out_unlock:
+	spin_unlock(src_ptl);
+	spin_unlock(dst_ptl);
+	return ret;
+}
+
 void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)
 {
 	pud_t entry;
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 4c1a2c02e13b..3a25166bdbc4 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3320,6 +3320,95 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 	return ret;
 }
 
+int copy_hugetlb_page_range_nocow(struct mm_struct *dst, struct mm_struct *src,
+			    struct vm_area_struct *vma)
+{
+	pte_t *src_pte, *dst_pte, entry;
+	struct page *ptepage;
+	unsigned long addr;
+	int cow;
+	struct hstate *h = hstate_vma(vma);
+	unsigned long sz = huge_page_size(h);
+	unsigned long mmun_start;	/* For mmu_notifiers */
+	unsigned long mmun_end;		/* For mmu_notifiers */
+	int ret = 0;
+
+	cow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
+
+	mmun_start = vma->vm_start;
+	mmun_end = vma->vm_end;
+#if 0
+	if (cow)
+		mmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);
+#endif
+	for (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {
+		spinlock_t *src_ptl, *dst_ptl;
+		src_pte = huge_pte_offset(src, addr, sz);
+		if (!src_pte)
+			continue;
+		dst_pte = huge_pte_alloc(dst, addr, sz);
+		if (!dst_pte) {
+			ret = -ENOMEM;
+			break;
+		}
+
+		/* If the pagetables are shared don't copy or take references */
+		if (dst_pte == src_pte)
+			continue;
+
+		dst_ptl = huge_pte_lock(h, dst, dst_pte);
+		src_ptl = huge_pte_lockptr(h, src, src_pte);
+		spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
+		entry = huge_ptep_get(src_pte);
+		if (huge_pte_none(entry)) { /* skip none entry */
+			;
+		} else if (unlikely(is_hugetlb_entry_migration(entry) ||
+				    is_hugetlb_entry_hwpoisoned(entry))) {
+			swp_entry_t swp_entry = pte_to_swp_entry(entry);
+
+#if 0
+			if (is_write_migration_entry(swp_entry) && cow) {
+				/*
+				 * COW mappings require pages in both
+				 * parent and child to be set to read.
+				 */
+				make_migration_entry_read(&swp_entry);
+				entry = swp_entry_to_pte(swp_entry);
+				set_huge_swap_pte_at(src, addr, src_pte,
+						     entry, sz);
+			}
+#endif
+			set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);
+		} else {
+#if 0
+			if (cow) {
+				/*
+				 * No need to notify as we are downgrading page
+				 * table protection not changing it to point
+				 * to a new page.
+				 *
+				 * See Documentation/vm/mmu_notifier.txt
+				 */
+				huge_ptep_set_wrprotect(src, addr, src_pte);
+			}
+#endif
+			entry = huge_ptep_get(src_pte);
+			ptepage = pte_page(entry);
+			get_page(ptepage);
+			page_dup_rmap(ptepage, true);
+			set_huge_pte_at(dst, addr, dst_pte, entry);
+			hugetlb_count_add(pages_per_huge_page(h), dst);
+		}
+		spin_unlock(src_ptl);
+		spin_unlock(dst_ptl);
+	}
+#if 0
+	if (cow)
+		mmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);
+#endif
+	return ret;
+}
+
 void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
 			    unsigned long start, unsigned long end,
 			    struct page *ref_page)
diff --git a/mm/memory.c b/mm/memory.c
index fe497cecd2ab..798fe8aa637c 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1059,6 +1059,130 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	return 0;
 }
 
+static inline unsigned long
+copy_one_pte_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,
+		unsigned long addr, int *rss)
+{
+	unsigned long vm_flags = vma->vm_flags;
+	pte_t pte = *src_pte;
+	struct page *page;
+
+	/* pte contains position in swap or file, so copy. */
+	if (unlikely(!pte_present(pte))) {
+		swp_entry_t entry = pte_to_swp_entry(pte);
+
+		if (likely(!non_swap_entry(entry))) {
+			if (swap_duplicate(entry) < 0)
+				return entry.val;
+
+			/* make sure dst_mm is on swapoff's mmlist. */
+			if (unlikely(list_empty(&dst_mm->mmlist))) {
+				spin_lock(&mmlist_lock);
+				if (list_empty(&dst_mm->mmlist))
+					list_add(&dst_mm->mmlist,
+							&src_mm->mmlist);
+				spin_unlock(&mmlist_lock);
+			}
+			rss[MM_SWAPENTS]++;
+		} else if (is_migration_entry(entry)) {
+			page = migration_entry_to_page(entry);
+
+			rss[mm_counter(page)]++;
+
+#if 0
+			if (is_write_migration_entry(entry) &&
+					is_cow_mapping(vm_flags)) {
+				/*
+				 * COW mappings require pages in both
+				 * parent and child to be set to read.
+				 */
+				make_migration_entry_read(&entry);
+				pte = swp_entry_to_pte(entry);
+				if (pte_swp_soft_dirty(*src_pte))
+					pte = pte_swp_mksoft_dirty(pte);
+				set_pte_at(src_mm, addr, src_pte, pte);
+			}
+#endif
+		} else if (is_device_private_entry(entry)) {
+			page = device_private_entry_to_page(entry);
+
+			/*
+			 * Update rss count even for unaddressable pages, as
+			 * they should treated just like normal pages in this
+			 * respect.
+			 *
+			 * We will likely want to have some new rss counters
+			 * for unaddressable pages, at some point. But for now
+			 * keep things as they are.
+			 */
+			get_page(page);
+			rss[mm_counter(page)]++;
+			page_dup_rmap(page, false);
+
+			/*
+			 * We do not preserve soft-dirty information, because so
+			 * far, checkpoint/restore is the only feature that
+			 * requires that. And checkpoint/restore does not work
+			 * when a device driver is involved (you cannot easily
+			 * save and restore device driver state).
+			 */
+#if 0
+			if (is_write_device_private_entry(entry) &&
+			    is_cow_mapping(vm_flags)) {
+				make_device_private_entry_read(&entry);
+				pte = swp_entry_to_pte(entry);
+				set_pte_at(src_mm, addr, src_pte, pte);
+			}
+#endif
+		}
+		goto out_set_pte;
+	}
+
+	/*
+	 * If it's a COW mapping, write protect it both
+	 * in the parent and the child
+	 */
+#if 0
+	if (is_cow_mapping(vm_flags)) {
+		ptep_set_wrprotect(src_mm, addr, src_pte);
+		pte = pte_wrprotect(pte);
+	}
+#endif
+
+	/*
+	 * If it's a shared mapping, mark it clean in
+	 * the child
+	 */
+	if (vm_flags & VM_SHARED)
+		pte = pte_mkclean(pte);
+//	pte = pte_mkold(pte);
+
+	page = vm_normal_page(vma, addr, pte);
+	if (page) {
+		get_page(page);
+		page_dup_rmap(page, false);
+		rss[mm_counter(page)]++;
+	} else if (pte_devmap(pte)) {
+		page = pte_page(pte);
+
+		/*
+		 * Cache coherent device memory behave like regular page and
+		 * not like persistent memory page. For more informations see
+		 * MEMORY_DEVICE_CACHE_COHERENT in memory_hotplug.h
+		 */
+		if (is_device_public_page(page)) {
+			get_page(page);
+			page_dup_rmap(page, false);
+			rss[mm_counter(page)]++;
+		}
+	}
+
+out_set_pte:
+	set_pte_at(dst_mm, addr, dst_pte, pte);
+	return 0;
+}
+
 static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		   pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
 		   unsigned long addr, unsigned long end)
@@ -1122,6 +1246,69 @@ static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	return 0;
 }
 
+static int copy_pte_range_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		   pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
+		   unsigned long addr, unsigned long end)
+{
+	pte_t *orig_src_pte, *orig_dst_pte;
+	pte_t *src_pte, *dst_pte;
+	spinlock_t *src_ptl, *dst_ptl;
+	int progress = 0;
+	int rss[NR_MM_COUNTERS];
+	swp_entry_t entry = (swp_entry_t){0};
+
+again:
+	init_rss_vec(rss);
+
+	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
+	if (!dst_pte)
+		return -ENOMEM;
+	src_pte = pte_offset_map(src_pmd, addr);
+	src_ptl = pte_lockptr(src_mm, src_pmd);
+	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
+	orig_src_pte = src_pte;
+	orig_dst_pte = dst_pte;
+	arch_enter_lazy_mmu_mode();
+
+	do {
+		/*
+		 * We are holding two locks at this point - either of them
+		 * could generate latencies in another task on another CPU.
+		 */
+		if (progress >= 32) {
+			progress = 0;
+			if (need_resched() ||
+			    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))
+				break;
+		}
+		if (pte_none(*src_pte)) {
+			progress++;
+			continue;
+		}
+		entry.val = copy_one_pte_nocow(dst_mm, src_mm, dst_pte, src_pte,
+							vma, addr, rss);
+		if (entry.val)
+			break;
+		progress += 8;
+	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
+
+	arch_leave_lazy_mmu_mode();
+	spin_unlock(src_ptl);
+	pte_unmap(orig_src_pte);
+	add_mm_rss_vec(dst_mm, rss);
+	pte_unmap_unlock(orig_dst_pte, dst_ptl);
+	cond_resched();
+
+	if (entry.val) {
+		if (add_swap_count_continuation(entry, GFP_KERNEL) < 0)
+			return -ENOMEM;
+		progress = 0;
+	}
+	if (addr != end)
+		goto again;
+	return 0;
+}
+
 static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		pud_t *dst_pud, pud_t *src_pud, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end)
@@ -1156,6 +1343,40 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src
 	return 0;
 }
 
+static inline int copy_pmd_range_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pud_t *dst_pud, pud_t *src_pud, struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end)
+{
+	pmd_t *src_pmd, *dst_pmd;
+	unsigned long next;
+
+	dst_pmd = pmd_alloc(dst_mm, dst_pud, addr);
+	if (!dst_pmd)
+		return -ENOMEM;
+	src_pmd = pmd_offset(src_pud, addr);
+	do {
+		next = pmd_addr_end(addr, end);
+		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)
+			|| pmd_devmap(*src_pmd)) {
+			int err;
+			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);
+			err = copy_huge_pmd_nocow(dst_mm, src_mm,
+					    dst_pmd, src_pmd, addr, vma);
+			if (err == -ENOMEM)
+				return -ENOMEM;
+			if (!err)
+				continue;
+			/* fall through */
+		}
+		if (pmd_none_or_clear_bad(src_pmd))
+			continue;
+		if (copy_pte_range_nocow(dst_mm, src_mm, dst_pmd, src_pmd,
+						vma, addr, next))
+			return -ENOMEM;
+	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
+	return 0;
+}
+
 static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		p4d_t *dst_p4d, p4d_t *src_p4d, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end)
@@ -1190,6 +1411,40 @@ static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src
 	return 0;
 }
 
+static inline int copy_pud_range_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		p4d_t *dst_p4d, p4d_t *src_p4d, struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end)
+{
+	pud_t *src_pud, *dst_pud;
+	unsigned long next;
+
+	dst_pud = pud_alloc(dst_mm, dst_p4d, addr);
+	if (!dst_pud)
+		return -ENOMEM;
+	src_pud = pud_offset(src_p4d, addr);
+	do {
+		next = pud_addr_end(addr, end);
+		if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {
+			int err;
+
+			VM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, vma);
+			err = copy_huge_pud_nocow(dst_mm, src_mm,
+					    dst_pud, src_pud, addr, vma);
+			if (err == -ENOMEM)
+				return -ENOMEM;
+			if (!err)
+				continue;
+			/* fall through */
+		}
+		if (pud_none_or_clear_bad(src_pud))
+			continue;
+		if (copy_pmd_range_nocow(dst_mm, src_mm, dst_pud, src_pud,
+						vma, addr, next))
+			return -ENOMEM;
+	} while (dst_pud++, src_pud++, addr = next, addr != end);
+	return 0;
+}
+
 static inline int copy_p4d_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,
 		unsigned long addr, unsigned long end)
@@ -1212,6 +1467,28 @@ static inline int copy_p4d_range(struct mm_struct *dst_mm, struct mm_struct *src
 	return 0;
 }
 
+static inline int copy_p4d_range_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,
+		unsigned long addr, unsigned long end)
+{
+	p4d_t *src_p4d, *dst_p4d;
+	unsigned long next;
+
+	dst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);
+	if (!dst_p4d)
+		return -ENOMEM;
+	src_p4d = p4d_offset(src_pgd, addr);
+	do {
+		next = p4d_addr_end(addr, end);
+		if (p4d_none_or_clear_bad(src_p4d))
+			continue;
+		if (copy_pud_range_nocow(dst_mm, src_mm, dst_p4d, src_p4d,
+						vma, addr, next))
+			return -ENOMEM;
+	} while (dst_p4d++, src_p4d++, addr = next, addr != end);
+	return 0;
+}
+
 int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		struct vm_area_struct *vma)
 {
@@ -1279,6 +1556,66 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	return ret;
 }
 
+int copy_page_range_nocow(struct mm_struct *dst_mm, struct mm_struct *src_mm,
+		struct vm_area_struct *vma)
+{
+	pgd_t *src_pgd, *dst_pgd;
+	unsigned long next;
+	unsigned long addr = vma->vm_start;
+	unsigned long end = vma->vm_end;
+	unsigned long mmun_start;	/* For mmu_notifiers */
+	unsigned long mmun_end;		/* For mmu_notifiers */
+	int ret;
+
+	/*
+	 * Don't copy ptes where a page fault will fill them correctly.
+	 * Fork becomes much lighter when there are big shared or private
+	 * readonly mappings. The tradeoff is that copy_page_range is more
+	 * efficient than faulting.
+	 */
+	if (!(vma->vm_flags & (VM_HUGETLB | VM_PFNMAP | VM_MIXEDMAP)) &&
+			!vma->anon_vma)
+		return 0;
+
+	if (is_vm_hugetlb_page(vma))
+		return copy_hugetlb_page_range_nocow(dst_mm, src_mm, vma);
+
+	if (unlikely(vma->vm_flags & VM_PFNMAP)) {
+		/*
+		 * We do not free on error cases below as remove_vma
+		 * gets called on error from higher level routine
+		 */
+		ret = track_pfn_copy(vma);
+		if (ret)
+			return ret;
+	}
+
+	/*
+	 * We need to invalidate the secondary MMU mappings only when
+	 * there could be a permission downgrade on the ptes of the
+	 * parent mm. And a permission downgrade will only happen if
+	 * is_cow_mapping() returns true.
+	 */
+	mmun_start = addr;
+	mmun_end   = end;
+
+	ret = 0;
+	dst_pgd = pgd_offset(dst_mm, addr);
+	src_pgd = pgd_offset(src_mm, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		if (pgd_none_or_clear_bad(src_pgd))
+			continue;
+		if (unlikely(copy_p4d_range_nocow(dst_mm, src_mm, dst_pgd, src_pgd,
+					    vma, addr, next))) {
+			ret = -ENOMEM;
+			break;
+		}
+	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
+
+	return ret;
+}
+
 static unsigned long zap_pte_range(struct mmu_gather *tlb,
 				struct vm_area_struct *vma, pmd_t *pmd,
 				unsigned long addr, unsigned long end,
-- 
2.19.0.rc1


From 6225e7a63b6e4b1bfe60b8873c77dacab62faad3 Mon Sep 17 00:00:00 2001
From: Yang Bo <bo@hyper.sh>
Date: Wed, 26 Sep 2018 16:11:00 +0800
Subject: [PATCH 6/6] Switch to dup_mm_nocow.

as_copy DO NOT need cow, actually it needs NOT_COPY_ON_WRITE to work
properly.

Signed-off-by: Yang Bo <bo@hyper.sh>
---
 mm/as.c | 37 ++++++++++++++++++++++++++++---------
 1 file changed, 28 insertions(+), 9 deletions(-)

diff --git a/mm/as.c b/mm/as.c
index fae2b7da58b4..1c99d1492a83 100644
--- a/mm/as.c
+++ b/mm/as.c
@@ -97,12 +97,12 @@ SYSCALL_DEFINE0(as_create)
  * */
 SYSCALL_DEFINE0(as_copy)
 {
-    int fd, ret;
+    int fd, ret __attribute__((__unused__));
     struct mm_struct *mm;
     struct task_struct *task;
 
     task = current;
-    mm = dup_mm(task);
+    mm = dup_mm_nocow(task);
     if (!mm) {
         return -ENOMEM;
     }
@@ -127,12 +127,14 @@ SYSCALL_DEFINE0(as_copy)
 #endif
     spin_unlock(&mmlist_lock);
 
+#if 0
 #ifdef DEBUG
     ret = do_as_switch_mm(fd);
     if (ret) {
         __close_fd(current->files, fd);
         return ret;
     }
+#endif
 #endif
 
     return fd;
@@ -200,12 +202,16 @@ static long do_as_switch_mm(unsigned int fd)
     unsigned long oldfs2, oldgs;
     unsigned int fsindex, gsindex;
 
+//#undef SET_REGS
+#define SET_REGS
+#ifdef SET_REGS
     oldfs = get_fs();
     rdmsrl(MSR_FS_BASE, oldfs2);
     rdmsrl(MSR_KERNEL_GS_BASE, oldgs);
     savesegment(fs, fsindex);
     savesegment(gs, gsindex);
-#ifdef DEBUG
+#endif
+#if defined(DEBUG) && defined(SET_REGS)
     printk(KERN_INFO "oldfs[%lx], fsbase[%lx], gsbase[%lx]\n", oldfs.seg,
             oldfs2, oldgs);
     printk(KERN_INFO "fsindex[%x], gsindex[%x]\n", fsindex, gsindex);
@@ -271,12 +277,20 @@ static long do_as_switch_mm(unsigned int fd)
     mmdrop(active_mm);
 
 out:
-//    set_fs(oldfs);
-    set_fs(USER_DS);
-    wrmsrl(MSR_FS_BASE, oldfs2);
-    wrmsrl(MSR_KERNEL_GS_BASE, oldgs);
+#ifdef SET_REGS
+    /* Not sure why we need this..
+     * lldt will not affect fs and gs..
+     * maybe loadsegment(fs, 0) and load_gs_index(0) in
+     * deactivate_mm() affects this? I don't know, just need
+     * to set these to work. 
+     * */
+    set_fs(oldfs);
+//    set_fs(USER_DS);
     loadsegment(fs, fsindex);
+    wrmsrl(MSR_FS_BASE, oldfs2);
     load_gs_index(gsindex);
+    wrmsrl(MSR_KERNEL_GS_BASE, oldgs);
+#endif
     return 0;
 }
 
@@ -312,8 +326,8 @@ static int as_release(struct inode *inode, struct file *file)
 #ifdef DEBUG
 static void dump_pt_regs(struct pt_regs *reg)
 {
-    int i = 0;
-    char *c, ch;
+    int i __attribute__((__unused__)) = 0;
+    char *c __attribute__((__unused__)), ch __attribute__((__unused__));
     printk(KERN_INFO "r15[%lx], r14[%lx], r13[%lx], r12[%lx]\n",
             reg->r15, reg->r14, reg->r13, reg->r12);
     printk(KERN_INFO "bp[%lx], bx[%lx], r11[%lx], r10[%lx]\n",
@@ -325,6 +339,11 @@ static void dump_pt_regs(struct pt_regs *reg)
     printk(KERN_INFO "ip[%lx], cs[%lx], flags[%lx], sp[%lx]\n",
             reg->ip, reg->cs, reg->flags, reg->sp);
     printk(KERN_INFO "ss[%lx]\n", reg->ss);
+
+    printk(KERN_INFO "thread fsindex[%x], thread gsindex[%x], "
+                     "thread fsbase[%lx], thread gsbase[%lx]\n", 
+                     current->thread.fsindex, current->thread.gsindex,
+                     current->thread.fsbase, current->thread.gsbase);
 #if 0
     printk(KERN_INFO "dump instructions:\n");
     c = (char *)reg->ip;
-- 
2.19.0.rc1

